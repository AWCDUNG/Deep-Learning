{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8vHzXKo73X3",
        "outputId": "1734a131-cbd8-4700-ae6d-ef3377c705a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tsN-ar17zl7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOAD DATA\n"
      ],
      "metadata": {
        "id": "X_hiyUB1GAmr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpHg7DYC7zoF",
        "outputId": "ca9ece77-e3d2-4d32-967e-97fa4b91b30f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of x_train: (3251, 224, 224, 3)\n",
            "Shape of x_test: (1775, 224, 224, 3)\n",
            "Shape of y_train: (3251,)\n",
            "Shape of y_test: (1775,)\n",
            "Training data category sizes:\n",
            "Afghan_hound: 298\n",
            "Blenheim_spaniel: 397\n",
            "Chihuahua: 387\n",
            "Japanese_spaniel: 300\n",
            "Maltese_dog: 381\n",
            "papillon: 326\n",
            "Pekinese: 257\n",
            "Rhodesian_ridgeback: 306\n",
            "Shih-Tzu: 329\n",
            "toy_terrier: 270\n",
            "Test data category sizes:\n",
            "Afghan_hound: 162\n",
            "Blenheim_spaniel: 252\n",
            "Chihuahua: 153\n",
            "Japanese_spaniel: 214\n",
            "Maltese_dog: 102\n",
            "papillon: 184\n",
            "Pekinese: 184\n",
            "Rhodesian_ridgeback: 181\n",
            "Shih-Tzu: 185\n",
            "toy_terrier: 158\n"
          ]
        }
      ],
      "source": [
        "train_data_dir = \"/content/drive/MyDrive/Đồ Án/DataSet/dog-20240603T032219Z-001/dog/10dogtrain\"\n",
        "test_data_dir = \"/content/drive/MyDrive/Đồ Án/DataSet/dog-20240603T032219Z-001/dog/10dogtest\"\n",
        "# Define the dog categories\n",
        "categories = [\"Afghan_hound\", \"Blenheim_spaniel\", \"Chihuahua\", \"Japanese_spaniel\", \"Maltese_dog\",\n",
        "              \"papillon\", \"Pekinese\", \"Rhodesian_ridgeback\", \"Shih-Tzu\", \"toy_terrier\"]\n",
        "\n",
        "\n",
        "train_category_sizes = {category: 0 for category in categories}\n",
        "test_category_sizes = {category: 0 for category in categories}\n",
        "\n",
        "# Create empty lists for training data and labels\n",
        "x_train = []\n",
        "y_train = []\n",
        "# Iterate through each dog category\n",
        "for category in categories:\n",
        "    # Path to each dog category\n",
        "    category_path = os.path.join(train_data_dir, category)\n",
        "    # Iterate through each image in that category\n",
        "    for img in os.listdir(category_path):\n",
        "        try:\n",
        "            img_path = os.path.join(category_path, img)\n",
        "            # Read and resize the image\n",
        "            img = Image.open(img_path)\n",
        "            img_resized = img.resize((224, 224))  # Resize to 224x224\n",
        "            img_array = np.array(img_resized)\n",
        "            # Add the image and label to the lists\n",
        "            x_train.append(img_array)\n",
        "            y_train.append(categories.index(category))\n",
        "            train_category_sizes[category] += 1\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {img_path}: {e}\")\n",
        "# Convert lists to NumPy arrays\n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "# Repeat the process for test data\n",
        "x_test = []\n",
        "y_test = []\n",
        "for category in categories:\n",
        "    category_path = os.path.join(test_data_dir, category)\n",
        "    for img in os.listdir(category_path):\n",
        "        try:\n",
        "            img_path = os.path.join(category_path, img)\n",
        "            img = Image.open(img_path)\n",
        "            img_resized = img.resize((224, 224))  # Resize to 224x224\n",
        "            img_array = np.array(img_resized)\n",
        "            x_test.append(img_array)\n",
        "            y_test.append(categories.index(category))\n",
        "            test_category_sizes[category] += 1\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {img_path}: {e}\")\n",
        "x_test = np.array(x_test)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "print(f\"Shape of x_train: {x_train.shape}\")\n",
        "print(f\"Shape of x_test: {x_test.shape}\")\n",
        "print(f\"Shape of y_train: {y_train.shape}\")\n",
        "print(f\"Shape of y_test: {y_test.shape}\")\n",
        "\n",
        "print(\"Training data category sizes:\")\n",
        "for category, size in train_category_sizes.items():\n",
        "    print(f\"{category}: {size}\")\n",
        "print(\"Test data category sizes:\")\n",
        "for category, size in test_category_sizes.items():\n",
        "    print(f\"{category}: {size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRANFERS LEARNING\n"
      ],
      "metadata": {
        "id": "Klw-jSKpGLWT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7YlKH-T7zqH"
      },
      "outputs": [],
      "source": [
        "# Data augmentation and preprocessing\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip('horizontal'),\n",
        "    tf.keras.layers.RandomRotation(0.2),\n",
        "    tf.keras.layers.RandomZoom(0.2),\n",
        "    tf.keras.layers.RandomContrast(0.2),\n",
        "    tf.keras.layers.Resizing(224, 224)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKgck7o27zr8",
        "outputId": "99d04873-1d99-4fa0-cd73-92e405a1a391"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 1s 0us/step\n"
          ]
        }
      ],
      "source": [
        "preprocess_input = tf.keras.applications.resnet50.preprocess_input\n",
        "IMG_SIZE = (224, 224)\n",
        "IMG_SHAPE = IMG_SIZE + (3,)\n",
        "\n",
        "base_model = tf.keras.applications.ResNet50(input_shape=IMG_SHAPE,\n",
        "                                            include_top=False,\n",
        "                                            weights='imagenet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Euiwe54_7zv8"
      },
      "outputs": [],
      "source": [
        "base_model.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQL92rCg7z0E"
      },
      "outputs": [],
      "source": [
        "# Build the model\n",
        "inputs = tf.keras.Input(shape=(224, 224, 3))\n",
        "x = data_augmentation(inputs)\n",
        "x = preprocess_input(x)\n",
        "x = base_model(x, training=False)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = tf.keras.layers.Dropout(0.3)(x)\n",
        "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
        "x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
        "x = tf.keras.layers.Dense(32, activation='relu')(x)\n",
        "outputs = tf.keras.layers.Dense(10, activation='softmax')(x)  # 10 classes\n",
        "\n",
        "\n",
        "model = tf.keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMj3CKM97z2O"
      },
      "outputs": [],
      "source": [
        "base_learning_rate = 0.001\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "              metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghOEqNhP7z4W"
      },
      "outputs": [],
      "source": [
        "# Callbacks\n",
        "early_stopping_cb = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "reduce_lr_cb = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.00001)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZ8URRXk7z6Z",
        "outputId": "0b8ac0eb-5515-4349-97ba-e5ae28f966fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "102/102 [==============================] - 1073s 11s/step - loss: 1.1008 - accuracy: 0.6373 - val_loss: 0.3594 - val_accuracy: 0.8980 - lr: 0.0010\n",
            "Epoch 2/15\n",
            "102/102 [==============================] - 1021s 10s/step - loss: 0.5303 - accuracy: 0.8207 - val_loss: 0.3074 - val_accuracy: 0.9003 - lr: 0.0010\n",
            "Epoch 3/15\n",
            "102/102 [==============================] - 1013s 10s/step - loss: 0.5204 - accuracy: 0.8237 - val_loss: 0.3004 - val_accuracy: 0.8975 - lr: 0.0010\n",
            "Epoch 4/15\n",
            "102/102 [==============================] - 1014s 10s/step - loss: 0.4339 - accuracy: 0.8487 - val_loss: 0.2560 - val_accuracy: 0.9206 - lr: 0.0010\n",
            "Epoch 5/15\n",
            "102/102 [==============================] - 955s 9s/step - loss: 0.4065 - accuracy: 0.8607 - val_loss: 0.2741 - val_accuracy: 0.9166 - lr: 0.0010\n",
            "Epoch 6/15\n",
            "102/102 [==============================] - 955s 9s/step - loss: 0.3420 - accuracy: 0.8850 - val_loss: 0.2760 - val_accuracy: 0.9161 - lr: 0.0010\n",
            "Epoch 7/15\n",
            "102/102 [==============================] - 1007s 10s/step - loss: 0.3231 - accuracy: 0.8886 - val_loss: 0.2555 - val_accuracy: 0.9161 - lr: 2.0000e-04\n",
            "Epoch 8/15\n",
            "102/102 [==============================] - 947s 9s/step - loss: 0.2940 - accuracy: 0.8976 - val_loss: 0.2428 - val_accuracy: 0.9262 - lr: 2.0000e-04\n",
            "Epoch 9/15\n",
            "102/102 [==============================] - 952s 9s/step - loss: 0.2940 - accuracy: 0.8982 - val_loss: 0.2392 - val_accuracy: 0.9256 - lr: 2.0000e-04\n",
            "Epoch 10/15\n",
            " 10/102 [=>............................] - ETA: 9:26 - loss: 0.2421 - accuracy: 0.9125"
          ]
        }
      ],
      "source": [
        "# Fit the model\n",
        "history = model.fit(x_train, y_train, epochs=15, validation_data=(x_test, y_test), callbacks=[early_stopping_cb, reduce_lr_cb])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OeMYwnv07z8Z"
      },
      "outputs": [],
      "source": [
        "base_model.trainable = True"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FINE TUNNING"
      ],
      "metadata": {
        "id": "JAhoRWcFGX9e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khP8lsuF7z-Z"
      },
      "outputs": [],
      "source": [
        "# Unfreeze the top layers of the model\n",
        "fine_tune_at = 100  # Adjust based on ResNet50 structure\n",
        "for layer in base_model.layers[:fine_tune_at]:\n",
        "    layer.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGuogoHm70Cn"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate/10),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLDQs9xo9zyv"
      },
      "outputs": [],
      "source": [
        "fine_tune_epochs = 10\n",
        "total_epochs = len(history.epoch) + fine_tune_epochs\n",
        "\n",
        "history_fine = model.fit(x_train, y_train,\n",
        "                         epochs=total_epochs,\n",
        "                         initial_epoch=len(history.epoch),\n",
        "                         validation_data=(x_test, y_test),\n",
        "                         callbacks=[early_stopping_cb, reduce_lr_cb])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AM40xHsC9zwk"
      },
      "outputs": [],
      "source": [
        "# Plot the training and validation accuracy and loss side by side\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Plot the training and validation accuracy\n",
        "ax1.plot(history.history['accuracy'])\n",
        "ax1.plot(history.history['val_accuracy'])\n",
        "ax1.set_title('Model Accuracy')\n",
        "ax1.set_ylabel('Accuracy')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "# Plot the training and validation loss\n",
        "ax2.plot(history.history['loss'])\n",
        "ax2.plot(history.history['val_loss'])\n",
        "ax2.set_title('Model Loss')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "# Show the plots\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lg_ztyMq-cAQ"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8luoLKCd9zuM"
      },
      "outputs": [],
      "source": [
        "# Save the model to a .h5 file\n",
        "model.save('resnet50_model.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eoLgRpQo9zra"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(x_test)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Find incorrectly predicted images\n",
        "incorrect_indices = np.where(predicted_labels != y_test)[0]\n",
        "plt.figure(figsize=(20, 20))\n",
        "for i, incorrect_idx in enumerate(incorrect_indices[:30]):\n",
        "    plt.subplot(10, 10, i + 1)\n",
        "    plt.imshow(x_test[incorrect_idx])\n",
        "    plt.title(f\"True: {y_test[incorrect_idx]}, Pred: {predicted_labels[incorrect_idx]}\")\n",
        "    plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDxF8M18M458"
      },
      "outputs": [],
      "source": [
        "# prompt: plot dự đoán đúng sai\n",
        "\n",
        "# Plot the first 25 correct predictions\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5, 5, i + 1)\n",
        "    plt.imshow(x_test[i])\n",
        "    plt.title(f\"True: {y_test[i]}, Pred: {predicted_labels[i]}\")\n",
        "    plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Plot the first 25 incorrect predictions\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(25):\n",
        "    incorrect_idx = incorrect_indices[i]\n",
        "    plt.subplot(5, 5, i + 1)\n",
        "    plt.imshow(x_test[incorrect_idx])\n",
        "    plt.title(f\"True: {y_test[incorrect_idx]}, Pred: {predicted_labels[incorrect_idx]}\")\n",
        "    plt.axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsCIYkvFM4w5"
      },
      "outputs": [],
      "source": [
        "# prompt: plot dự đoán đúng sai\n",
        "\n",
        "# Plot the first 25 correct predictions\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5, 5, i + 1)\n",
        "    plt.imshow(x_test[i])\n",
        "    plt.title(f\"True: {y_test[i]}, Pred: {predicted_labels[i]}\")\n",
        "    plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Plot the first 25 incorrect predictions\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(25):\n",
        "    incorrect_idx = incorrect_indices[i]\n",
        "    plt.subplot(5, 5, i + 1)\n",
        "    plt.imshow(x_test[incorrect_idx])\n",
        "    plt.title(f\"True: {y_test[incorrect_idx]}, Pred: {predicted_labels[incorrect_idx]}\")\n",
        "    plt.axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TEST\n"
      ],
      "metadata": {
        "id": "x4m6y_v7Gmk6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYXG9Fd19znk"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cm = confusion_matrix(y_test, predicted_labels)\n",
        "# Normalize the confusion matrix\n",
        "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "# Set up the plot\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "\n",
        "# Create a heatmap and colorbar\n",
        "cax = ax.matshow(cm, cmap=plt.cm.Blues)\n",
        "fig.colorbar(cax)\n",
        "\n",
        "# Set labels\n",
        "ax.set(\n",
        "    title=\"Confusion Matrix (Normalized to %)\",\n",
        "    xlabel=\"True label\",\n",
        "    ylabel=\"Predicted label\",\n",
        "    xticks=np.arange(len(categories)),\n",
        "    yticks=np.arange(len(categories)),\n",
        "    xticklabels=categories,\n",
        "    yticklabels=categories,\n",
        ")\n",
        "\n",
        "# Label each cell with the corresponding value\n",
        "for (i, j), z in np.ndenumerate(cm_norm):\n",
        "    ax.text(j, i, f\"{z * 100:.1f}%\", ha=\"center\", va=\"center\", fontsize=10)\n",
        "\n",
        "# Rotate x-axis labels to prevent overlapping\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}